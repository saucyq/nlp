{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logo/0x150/1643104191/logo-mse.png)\n",
    "\n",
    "# AdvNLP Lab 1: Text Segmentation with NLTK\n",
    "\n",
    "## 1. Introduction\n",
    "The goal of the first AdvNLP lab is to run simple operations for text analysis using the [NLTK](http://www.nltk.org/) toolkit.  You will use the environment that you set up following the instructions of the previous notebook: [Python 3](https://www.python.org/) with [Jupyter](https://jupyter.org/) notebooks.  \n",
    "\n",
    "You will use NLTK functions to get texts from the web and segment (split) them into sentences and words (also called *tokens*).  You will also experiment with extracting statistics about the texts.  In the end, you will compare statistics of texts in two languages. \n",
    "\n",
    "To submit your practical work, please execute anew all cells of this notebook, then save it and submit it as homework on Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK: the Natural Language (Processing) Toolkit**\n",
    "\n",
    "Please add NLTK to your Python installation, by following the installation instructions at the [NLTK website](http://www.nltk.org/install.html).  A good way to get started is to look at [Chapter 1](http://www.nltk.org/book/ch01.html) of the [NLTK book (NLP with Python)](http://www.nltk.org/book/) and to try some of the instructions there.  \n",
    "\n",
    "The online edition is updated for Python 3, but the printed book, also available in PDF on some websites, is only for Python 2 ([Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit, Steven Bird, Ewan Klein, and Edward Loper, O'Reilly Media, 2009](http://shop.oreilly.com/product/9780596516499.do)). \n",
    "\n",
    "To use NLTK in Jupyter, all you need is to `import nltk` before you need it.  You must use the prefix `nltk.` unless you write for instance: `from nltk.book import *` which will import and define several text collections (a.k.a corpora).  NLTK can download from the associated website a large number of corpora.  NLTK has a download manager which can be called from a Python interpreter (not a notebook) using `nltk.download()`.  In this practical session, we will not use any of these."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:16:21.673483Z",
     "start_time": "2025-02-20T08:16:21.610362Z"
    }
   },
   "source": [
    "import nltk\n",
    "from nltk import bigrams\n",
    "from regex import split\n",
    "from soupsieve.util import lower\n",
    "\n",
    "nltk.download('punkt') # execute only once after installing NLTK"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\quent\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**1a.** To verify your NLTK library, please define an *array* of words called `sentence1` and use `nltk.bigrams` to generate all bigrams from it, i.e. pairs of consecutive words, sorted alphabetically.  You can see an example in [Sec. 3.3 of Ch. 1 of the NLTK book](http://www.nltk.org/book/ch01.html#collocations-and-bigrams). "
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:21:03.380474Z",
     "start_time": "2025-02-20T08:21:03.367279Z"
    }
   },
   "source": [
    "sentence1 =[\"I\",\"do\",\"this\",\"lab\",\"on\",\"thursday\"]\n",
    "sentence1 = [x.lower() for x in sentence1]\n",
    "print(list(bigrams(sorted(sentence1))))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('do', 'i'), ('i', 'lab'), ('lab', 'on'), ('on', 'this'), ('this', 'thursday')]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using NLTK to tokenize a text\n",
    "\n",
    "**2a.** Using inspiration from [Chapter 3 (3.1. Processing Raw Text) of the NLTK book](http://www.nltk.org/book/ch03.html), get a book from the Gutenberg Project in text format.   What is its length? Are these bytes or characters? If you are curious about \"special\" characters, you can refer to [Python's documentation of Unicode support](https://docs.python.org/3.8/howto/unicode.html)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:22:39.342509Z",
     "start_time": "2025-02-20T08:22:39.331836Z"
    }
   },
   "source": [
    "from urllib import request # you may need to run:  !pip install urllib"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:28:28.015147Z",
     "start_time": "2025-02-20T08:28:27.507987Z"
    }
   },
   "source": [
    "# Please write your Python code below and execute it.\n",
    "url = \"https://www.gutenberg.org/cache/epub/49813/pg49813.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "type(raw)\n",
    "len(raw)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58782"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:47:05.592081Z",
     "start_time": "2025-02-20T08:47:05.586516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"char: \",len(raw))\n",
    "#as the text is in french, they may use multi byte long character\n",
    "print(\"byte: \",len(raw.encode('utf-8')),\"difference byte vs char :\",len(raw.encode('utf-8'))-len(raw))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char:  58782\n",
      "byte:  59754 difference byte vs char : 972\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b.** We now want to keep only the meaningful text from the book, without the header and the final license. Determine, either by spotting the position of certain initial and final strings, or by trial and error, how much your should trim from the beginning and from the end in order to keep only the actual text of the book. Then save the result into a new string, and display its length."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T09:00:42.932078Z",
     "start_time": "2025-02-20T09:00:42.928840Z"
    }
   },
   "source": [
    "# Please write your Python code below and execute it.\n",
    "clair_dune = raw.split(\"***\")[2]\n",
    "print(\"char: \",len(clair_dune))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char:  38905\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has two useful functions, one to segment a text into sentences, and the other one to segment a text into words.  Usually, only word segmentation is called *tokenization*, but NLTK uses this name for both functions.\n",
    "\n",
    "* `nltk.sent_tokenize(...)` (documented [here](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.word_tokenize))\n",
    "* `nltk.word_tokenize(...)` (documented [here](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.sent_tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c.** Please segment the text above into sentences with NLTK, display the number of sentences, and display five sentences.  Please comment briefly about the quality of the segmentation.  If you think that some special characters degrade the results, please go back and replace them in the full text (e.g. with `.replace('s1', 's2')`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2d.** Please segment each sentence into tokens, store the result in a new variable (a list of lists), and display the same five sentences as above.  Please comment briefly on the quality of the tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2e.** Please display the total number of tokens in the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2f.** Please tokenize now directly the initial full text, without segmenting it into sentences.  Please display the total number of tokens found, and compare this number with the one obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2g.** Find the size of the vocabulary of your text (the unique *word types*) by converting the list of words (the *tokens*) to a Python `set`.  Note that these *types* include punctuations and other symbols found through tokenization, and that upper/lower case letters are different.  Display all words longer than 15 characters and not containing a hyphen (-)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2h.** What is the type-token ratio (TTR) of your text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Computing statistics with NLTK\n",
    "In this section, you will create a `nltk.Text` object which will enable you to compute statistics using NLTK functions.  [Chapter 1 of the NLTK book](http://www.nltk.org/book/ch01.html) provides examples of use for such objects.\n",
    "\n",
    "A `nltk.Text` can store one of the following text formats: (1) string, (2) list of all words (list of strings), (3) list of all sentences (list of lists of strings).  However, only option (2) will allow you to use counting methods.   Note that `nltk.word_tokenize()` and `nltk.sent_tokenize()` apply to strings but not to `ntlk.Text` objects (even when they store a string)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a.** Please create a `nltk.Text` object from the tokenized version of your text (without sentence segmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK can compute word frequencies for a given text, yielding a new object called a frequency distribution (`FreqDist`): see [Sec. 3.1 of Ch. 1 of the NLTK book](http://www.nltk.org/book/ch01.html#frequency-distributions).  Using such an object, we can get the most frequent words.  \n",
    "\n",
    "**3b.** Please construct the frequency distribution of your text.  Then, display the words that have at least 4 characters among the 50 most frequent words (calling the `most_common` method on the `FreqDist`), together with their number of occurrences.  Please comment briefly on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing Zipf's law\n",
    "\n",
    "It has been observed, for large amounts of texts, that when ranking words by decreasing frequencies, the ranks (1, 2, 3, ...) and the numbers of occurrences (say, 948, 321, 146, ...) follow a particular law called [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law).  Specifically, if *x&nbsp;* are the ranks and *y&nbsp;* the corresponding numbers of occurrences, their relation is approximately *y = a / x^b* for appropriate values of *a* and *b*.  If plotted in log-log coordinates, such a relation results in a linear plot (log(*y*) = *a* - *b* log(*x*)).\n",
    "\n",
    "**4a.** Using the `FreqDist` object, please collect the frequencies of the 1000 most frequent words, rank them by decreasing values, and plot the (rank, frequency) curve on a log-log scale (simply using the `.xscale(\"log\")` parameter of the plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before using matplotlib to display graphs inline, you must execute \n",
    "# the following two lines (assuming you have installed the library).\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b.** Please try to find manually the best values of a and b, so that the curve *y = a / x^b* (in other words (*x*, *a / x^b*)) matches as closely as possible the (frequency, rank) curve on the log-log scale.  You can use a trial-and-error approach, starting e.g. from *a* = 1000 and *b* = 1 (these values depend on the text that you tokenized).  Please display both curves on the same graph below to show how close they are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4c.** Please find automatically the best values of a and b using `scipy.optimize.curve_fit()`.  The best values are those leading to the closest fitting line (by default according to least squares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Does an undeciphered manuscript obey Zipf's law?\n",
    "\n",
    "The Voynich manuscript is an undeciphered manuscript from the 15th century: its script and language are still unknown, and it is not impossible that it is a hoax.  You can read about it [on Wikipedia](https://en.wikipedia.org/wiki/Voynich_manuscript) or even [on a dedicated website](http://www.voynich.nu/), which provides pictures and transcriptions.  A version of it converted to ASCII characters, one word per line, is made available on Moodle as the `voynich.txt` file (all characters correspond to symbols found in the manuscript).\n",
    "\n",
    "**5a.** Please compute the number of tokens, the number of types, and the type-to-token ration (TTR) for this document.  How does it compare with your previous text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5b.** Please examine if the tokens in the Voynich manuscript follow Zipf's law.  Specifically, please draw the (_rank_, _frequency_) curve in log-log scale for the 100 most frequent tokens, along with the closest (*x*, *a* / *x^b*) curve, and indicate the best values of *a* and *b*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code in this cell and execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5c.** What are your conclusions regarding Voynich's manuscript?  Is it likely to be similar to a real text in an unknown language?  In your answer, consider the values of TTR, the two parameters *a* and *b*, and the fitting of the *y = a / x^b* curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of AdvNLP Lab 1\n",
    "Please cleand and save the completed notebook, and upload it to Moodle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
