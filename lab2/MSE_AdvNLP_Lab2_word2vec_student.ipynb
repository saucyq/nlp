{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "quick-polls",
   "metadata": {},
   "source": [
    "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logo/0x150/1643104191/logo-mse.png)\n",
    "\n",
    "# AdvNLP Lab 2: Testing a pretrained word2vec model on analogy tasks\n",
    "\n",
    "**Objectives:**  experiment with *word vectors* from word2vec: test them on analogy tasks; use *accuracy and MRR* scores.\n",
    "\n",
    "**Useful documentation:** the [section on KeyedVectors in Gensim](https://radimrehurek.com/gensim/models/keyedvectors.html) and possibly the [section on word2vec](https://radimrehurek.com/gensim/models/word2vec.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-chase",
   "metadata": {},
   "source": [
    "## 1. Word2vec model trained on Google News\n",
    "**1a.** Please install the latest version of Gensim, preferably in a Conda environment. "
   ]
  },
  {
   "cell_type": "code",
   "id": "extreme-birthday",
   "metadata": {},
   "source": [
    "# !pip install --upgrade gensim\n",
    "# You can run the following verification:\n",
    "!pip show gensim"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "assumed-photographer",
   "metadata": {},
   "source": [
    "import gensim, os, random\n",
    "from gensim import downloader\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import utils\n",
    "# help(gensim.models.word2vec) # take a look if needed\n",
    "import time\n",
    "import itertools"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T10:57:46.145871Z",
     "start_time": "2025-02-27T10:57:45.743120Z"
    }
   },
   "cell_type": "code",
   "source": "help(gensim.models.word2vec)",
   "id": "ce4a521327adfd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module gensim.models.word2vec in gensim.models:\n",
      "\n",
      "NAME\n",
      "    gensim.models.word2vec\n",
      "\n",
      "DESCRIPTION\n",
      "    Introduction\n",
      "    ============\n",
      "\n",
      "    This module implements the word2vec family of algorithms, using highly optimized C routines,\n",
      "    data streaming and Pythonic interfaces.\n",
      "\n",
      "    The word2vec algorithms include skip-gram and CBOW models, using either\n",
      "    hierarchical softmax or negative sampling: `Tomas Mikolov et al: Efficient Estimation of Word Representations\n",
      "    in Vector Space <https://arxiv.org/pdf/1301.3781.pdf>`_, `Tomas Mikolov et al: Distributed Representations of Words\n",
      "    and Phrases and their Compositionality <https://arxiv.org/abs/1310.4546>`_.\n",
      "\n",
      "    Other embeddings\n",
      "    ================\n",
      "\n",
      "    There are more ways to train word vectors in Gensim than just Word2Vec.\n",
      "    See also :class:`~gensim.models.doc2vec.Doc2Vec`, :class:`~gensim.models.fasttext.FastText`.\n",
      "\n",
      "    The training algorithms were originally ported from the C package https://code.google.com/p/word2vec/\n",
      "    and extended with additional functionality and\n",
      "    `optimizations <https://rare-technologies.com/parallelizing-word2vec-in-python/>`_ over the years.\n",
      "\n",
      "    For a tutorial on Gensim word2vec, with an interactive web app trained on GoogleNews,\n",
      "    visit https://rare-technologies.com/word2vec-tutorial/.\n",
      "\n",
      "    Usage examples\n",
      "    ==============\n",
      "\n",
      "    Initialize a model with e.g.:\n",
      "\n",
      "    .. sourcecode:: pycon\n",
      "\n",
      "        >>> from gensim.test.utils import common_texts\n",
      "        >>> from gensim.models import Word2Vec\n",
      "        >>>\n",
      "        >>> model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
      "        >>> model.save(\"word2vec.model\")\n",
      "\n",
      "\n",
      "    **The training is streamed, so ``sentences`` can be an iterable**, reading input data\n",
      "    from the disk or network on-the-fly, without loading your entire corpus into RAM.\n",
      "\n",
      "    Note the ``sentences`` iterable must be *restartable* (not just a generator), to allow the algorithm\n",
      "    to stream over your dataset multiple times. For some examples of streamed iterables,\n",
      "    see :class:`~gensim.models.word2vec.BrownCorpus`,\n",
      "    :class:`~gensim.models.word2vec.Text8Corpus` or :class:`~gensim.models.word2vec.LineSentence`.\n",
      "\n",
      "    If you save the model you can continue training it later:\n",
      "\n",
      "    .. sourcecode:: pycon\n",
      "\n",
      "        >>> model = Word2Vec.load(\"word2vec.model\")\n",
      "        >>> model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)\n",
      "        (0, 2)\n",
      "\n",
      "    The trained word vectors are stored in a :class:`~gensim.models.keyedvectors.KeyedVectors` instance, as `model.wv`:\n",
      "\n",
      "    .. sourcecode:: pycon\n",
      "\n",
      "        >>> vector = model.wv['computer']  # get numpy vector of a word\n",
      "        >>> sims = model.wv.most_similar('computer', topn=10)  # get other similar words\n",
      "\n",
      "    The reason for separating the trained vectors into `KeyedVectors` is that if you don't\n",
      "    need the full model state any more (don't need to continue training), its state can be discarded,\n",
      "    keeping just the vectors and their keys proper.\n",
      "\n",
      "    This results in a much smaller and faster object that can be mmapped for lightning\n",
      "    fast loading and sharing the vectors in RAM between processes:\n",
      "\n",
      "    .. sourcecode:: pycon\n",
      "\n",
      "        >>> from gensim.models import KeyedVectors\n",
      "        >>>\n",
      "        >>> # Store just the words + their trained embeddings.\n",
      "        >>> word_vectors = model.wv\n",
      "        >>> word_vectors.save(\"word2vec.wordvectors\")\n",
      "        >>>\n",
      "        >>> # Load back with memory-mapping = read-only, shared across processes.\n",
      "        >>> wv = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')\n",
      "        >>>\n",
      "        >>> vector = wv['computer']  # Get numpy vector of a word\n",
      "\n",
      "    Gensim can also load word vectors in the \"word2vec C format\", as a\n",
      "    :class:`~gensim.models.keyedvectors.KeyedVectors` instance:\n",
      "\n",
      "    .. sourcecode:: pycon\n",
      "\n",
      "        >>> from gensim.test.utils import datapath\n",
      "        >>>\n",
      "        >>> # Load a word2vec model stored in the C *text* format.\n",
      "        >>> wv_from_text = KeyedVectors.load_word2vec_format(datapath('word2vec_pre_kv_c'), binary=False)\n",
      "        >>> # Load a word2vec model stored in the C *binary* format.\n",
      "        >>> wv_from_bin = KeyedVectors.load_word2vec_format(datapath(\"euclidean_vectors.bin\"), binary=True)\n",
      "\n",
      "    It is impossible to continue training the vectors loaded from the C format because the hidden weights,\n",
      "    vocabulary frequencies and the binary tree are missing. To continue training, you'll need the\n",
      "    full :class:`~gensim.models.word2vec.Word2Vec` object state, as stored by :meth:`~gensim.models.word2vec.Word2Vec.save`,\n",
      "    not just the :class:`~gensim.models.keyedvectors.KeyedVectors`.\n",
      "\n",
      "    You can perform various NLP tasks with a trained model. Some of the operations\n",
      "    are already built-in - see :mod:`gensim.models.keyedvectors`.\n",
      "\n",
      "    If you're finished training a model (i.e. no more updates, only querying),\n",
      "    you can switch to the :class:`~gensim.models.keyedvectors.KeyedVectors` instance:\n",
      "\n",
      "    .. sourcecode:: pycon\n",
      "\n",
      "        >>> word_vectors = model.wv\n",
      "        >>> del model\n",
      "\n",
      "    to trim unneeded model state = use much less RAM and allow fast loading and memory sharing (mmap).\n",
      "\n",
      "    Embeddings with multiword ngrams\n",
      "    ================================\n",
      "\n",
      "    There is a :mod:`gensim.models.phrases` module which lets you automatically\n",
      "    detect phrases longer than one word, using collocation statistics.\n",
      "    Using phrases, you can learn a word2vec model where \"words\" are actually multiword expressions,\n",
      "    such as `new_york_times` or `financial_crisis`:\n",
      "\n",
      "    .. sourcecode:: pycon\n",
      "\n",
      "        >>> from gensim.models import Phrases\n",
      "        >>>\n",
      "        >>> # Train a bigram detector.\n",
      "        >>> bigram_transformer = Phrases(common_texts)\n",
      "        >>>\n",
      "        >>> # Apply the trained MWE detector to a corpus, using the result to train a Word2vec model.\n",
      "        >>> model = Word2Vec(bigram_transformer[common_texts], min_count=1)\n",
      "\n",
      "    Pretrained models\n",
      "    =================\n",
      "\n",
      "    Gensim comes with several already pre-trained models, in the\n",
      "    `Gensim-data repository <https://github.com/RaRe-Technologies/gensim-data>`_:\n",
      "\n",
      "    .. sourcecode:: pycon\n",
      "\n",
      "        >>> import gensim.downloader\n",
      "        >>> # Show all available models in gensim-data\n",
      "        >>> print(list(gensim.downloader.info()['models'].keys()))\n",
      "        ['fasttext-wiki-news-subwords-300',\n",
      "         'conceptnet-numberbatch-17-06-300',\n",
      "         'word2vec-ruscorpora-300',\n",
      "         'word2vec-google-news-300',\n",
      "         'glove-wiki-gigaword-50',\n",
      "         'glove-wiki-gigaword-100',\n",
      "         'glove-wiki-gigaword-200',\n",
      "         'glove-wiki-gigaword-300',\n",
      "         'glove-twitter-25',\n",
      "         'glove-twitter-50',\n",
      "         'glove-twitter-100',\n",
      "         'glove-twitter-200',\n",
      "         '__testing_word2vec-matrix-synopsis']\n",
      "        >>>\n",
      "        >>> # Download the \"glove-twitter-25\" embeddings\n",
      "        >>> glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
      "        >>>\n",
      "        >>> # Use the downloaded vectors as usual:\n",
      "        >>> glove_vectors.most_similar('twitter')\n",
      "        [('facebook', 0.948005199432373),\n",
      "         ('tweet', 0.9403423070907593),\n",
      "         ('fb', 0.9342358708381653),\n",
      "         ('instagram', 0.9104824066162109),\n",
      "         ('chat', 0.8964964747428894),\n",
      "         ('hashtag', 0.8885937333106995),\n",
      "         ('tweets', 0.8878158330917358),\n",
      "         ('tl', 0.8778461217880249),\n",
      "         ('link', 0.8778210878372192),\n",
      "         ('internet', 0.8753897547721863)]\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        BrownCorpus\n",
      "        LineSentence\n",
      "        PathLineSentences\n",
      "        Text8Corpus\n",
      "    Heapitem(builtins.tuple)\n",
      "        Heapitem\n",
      "    gensim.utils.SaveLoad(builtins.object)\n",
      "        Word2Vec\n",
      "        Word2VecTrainables\n",
      "        Word2VecVocab\n",
      "\n",
      "    class BrownCorpus(builtins.object)\n",
      "     |  BrownCorpus(dirname)\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, dirname)\n",
      "     |      Iterate over sentences from the `Brown corpus <https://en.wikipedia.org/wiki/Brown_Corpus>`_\n",
      "     |      (part of `NLTK data <https://www.nltk.org/data.html>`_).\n",
      "     |\n",
      "     |  __iter__(self)\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "    class Heapitem(Heapitem)\n",
      "     |  Heapitem(count, index, left, right)\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      Heapitem\n",
      "     |      Heapitem\n",
      "     |      builtins.tuple\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __lt__(self, other)\n",
      "     |      Return self<value.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Heapitem:\n",
      "     |\n",
      "     |  __getnewargs__(self) from collections.Heapitem\n",
      "     |      Return self as a plain tuple.  Used by copy and pickle.\n",
      "     |\n",
      "     |  __repr__(self) from collections.Heapitem\n",
      "     |      Return a nicely formatted representation string\n",
      "     |\n",
      "     |  _asdict(self) from collections.Heapitem\n",
      "     |      Return a new dict which maps field names to their values.\n",
      "     |\n",
      "     |  _replace(self, /, **kwds) from collections.Heapitem\n",
      "     |      Return a new Heapitem object replacing specified fields with new values\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Heapitem:\n",
      "     |\n",
      "     |  _make(iterable) from collections.Heapitem\n",
      "     |      Make a new Heapitem object from a sequence or iterable\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from Heapitem:\n",
      "     |\n",
      "     |  __new__(_cls, count, index, left, right) from namedtuple_Heapitem.Heapitem\n",
      "     |      Create new instance of Heapitem(count, index, left, right)\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Heapitem:\n",
      "     |\n",
      "     |  count\n",
      "     |      Alias for field number 0\n",
      "     |\n",
      "     |  index\n",
      "     |      Alias for field number 1\n",
      "     |\n",
      "     |  left\n",
      "     |      Alias for field number 2\n",
      "     |\n",
      "     |  right\n",
      "     |      Alias for field number 3\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Heapitem:\n",
      "     |\n",
      "     |  __match_args__ = ('count', 'index', 'left', 'right')\n",
      "     |\n",
      "     |  _field_defaults = {}\n",
      "     |\n",
      "     |  _fields = ('count', 'index', 'left', 'right')\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.tuple:\n",
      "     |\n",
      "     |  __add__(self, value, /)\n",
      "     |      Return self+value.\n",
      "     |\n",
      "     |  __contains__(self, key, /)\n",
      "     |      Return bool(key in self).\n",
      "     |\n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |\n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |\n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |\n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |\n",
      "     |  __hash__(self, /)\n",
      "     |      Return hash(self).\n",
      "     |\n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |\n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |\n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |\n",
      "     |  __mul__(self, value, /)\n",
      "     |      Return self*value.\n",
      "     |\n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |\n",
      "     |  __rmul__(self, value, /)\n",
      "     |      Return value*self.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.tuple:\n",
      "     |\n",
      "     |  __class_getitem__(...)\n",
      "     |      See PEP 585\n",
      "\n",
      "    class LineSentence(builtins.object)\n",
      "     |  LineSentence(source, max_sentence_length=10000, limit=None)\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, source, max_sentence_length=10000, limit=None)\n",
      "     |      Iterate over a file that contains sentences: one line = one sentence.\n",
      "     |      Words must be already preprocessed and separated by whitespace.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      source : string or a file-like object\n",
      "     |          Path to the file on disk, or an already-open file object (must support `seek(0)`).\n",
      "     |      limit : int or None\n",
      "     |          Clip the file to the first `limit` lines. Do no clipping if `limit is None` (the default).\n",
      "     |\n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      .. sourcecode:: pycon\n",
      "     |\n",
      "     |          >>> from gensim.test.utils import datapath\n",
      "     |          >>> sentences = LineSentence(datapath('lee_background.cor'))\n",
      "     |          >>> for sentence in sentences:\n",
      "     |          ...     pass\n",
      "     |\n",
      "     |  __iter__(self)\n",
      "     |      Iterate through the lines in the source.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "    class PathLineSentences(builtins.object)\n",
      "     |  PathLineSentences(source, max_sentence_length=10000, limit=None)\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, source, max_sentence_length=10000, limit=None)\n",
      "     |      Like :class:`~gensim.models.word2vec.LineSentence`, but process all files in a directory\n",
      "     |      in alphabetical order by filename.\n",
      "     |\n",
      "     |      The directory must only contain files that can be read by :class:`gensim.models.word2vec.LineSentence`:\n",
      "     |      .bz2, .gz, and text files. Any file not ending with .bz2 or .gz is assumed to be a text file.\n",
      "     |\n",
      "     |      The format of files (either text, or compressed text files) in the path is one sentence = one line,\n",
      "     |      with words already preprocessed and separated by whitespace.\n",
      "     |\n",
      "     |      Warnings\n",
      "     |      --------\n",
      "     |      Does **not recurse** into subdirectories.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      source : str\n",
      "     |          Path to the directory.\n",
      "     |      limit : int or None\n",
      "     |          Read only the first `limit` lines from each file. Read all if limit is None (the default).\n",
      "     |\n",
      "     |  __iter__(self)\n",
      "     |      iterate through the files\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "    class Text8Corpus(builtins.object)\n",
      "     |  Text8Corpus(fname, max_sentence_length=10000)\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, fname, max_sentence_length=10000)\n",
      "     |      Iterate over sentences from the \"text8\" corpus, unzipped from https://mattmahoney.net/dc/text8.zip.\n",
      "     |\n",
      "     |  __iter__(self)\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "    class Word2Vec(gensim.utils.SaveLoad)\n",
      "     |  Word2Vec(sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      Word2Vec\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n",
      "     |      Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
      "     |\n",
      "     |      Once you're finished training a model (=no more updates, only querying)\n",
      "     |      store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in ``self.wv``\n",
      "     |      to reduce memory.\n",
      "     |\n",
      "     |      The full model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
      "     |      :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
      "     |\n",
      "     |      The trained word vectors can also be stored/loaded from a format compatible with the\n",
      "     |      original word2vec implementation via `self.wv.save_word2vec_format`\n",
      "     |      and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sentences : iterable of iterables, optional\n",
      "     |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      "     |          See also the `tutorial on data streaming in Python\n",
      "     |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      "     |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      "     |          in some other way.\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      "     |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      "     |      vector_size : int, optional\n",
      "     |          Dimensionality of the word vectors.\n",
      "     |      window : int, optional\n",
      "     |          Maximum distance between the current and predicted word within a sentence.\n",
      "     |      min_count : int, optional\n",
      "     |          Ignores all words with total frequency lower than this.\n",
      "     |      workers : int, optional\n",
      "     |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      "     |      sg : {0, 1}, optional\n",
      "     |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
      "     |      hs : {0, 1}, optional\n",
      "     |          If 1, hierarchical softmax will be used for model training.\n",
      "     |          If 0, hierarchical softmax will not be used for model training.\n",
      "     |      negative : int, optional\n",
      "     |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      "     |          should be drawn (usually between 5-20).\n",
      "     |          If 0, negative sampling will not be used.\n",
      "     |      ns_exponent : float, optional\n",
      "     |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      "     |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      "     |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      "     |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupr√©, Lesaint, & Royo-Letelier suggest that\n",
      "     |          other values may perform better for recommendation applications.\n",
      "     |      cbow_mean : {0, 1}, optional\n",
      "     |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      "     |      alpha : float, optional\n",
      "     |          The initial learning rate.\n",
      "     |      min_alpha : float, optional\n",
      "     |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      "     |      seed : int, optional\n",
      "     |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      "     |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      "     |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      "     |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      "     |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      "     |      max_vocab_size : int, optional\n",
      "     |          Limits the RAM during vocabulary building; if there are more unique\n",
      "     |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      "     |          Set to `None` for no limit.\n",
      "     |      max_final_vocab : int, optional\n",
      "     |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
      "     |          min_count is more than the calculated min_count, the specified min_count will be used.\n",
      "     |          Set to `None` if not required.\n",
      "     |      sample : float, optional\n",
      "     |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      "     |          useful range is (0, 1e-5).\n",
      "     |      hashfxn : function, optional\n",
      "     |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      "     |      epochs : int, optional\n",
      "     |          Number of iterations (epochs) over the corpus. (Formerly: `iter`)\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
      "     |          model.\n",
      "     |\n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |      sorted_vocab : {0, 1}, optional\n",
      "     |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
      "     |          See :meth:`~gensim.models.keyedvectors.KeyedVectors.sort_by_descending_frequency()`.\n",
      "     |      batch_words : int, optional\n",
      "     |          Target size (in words) for batches of examples passed to worker threads (and\n",
      "     |          thus cython routines).(Larger batches will be passed if individual\n",
      "     |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      "     |      compute_loss: bool, optional\n",
      "     |          If True, computes and stores loss value which can be retrieved using\n",
      "     |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      "     |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      "     |          Sequence of callbacks to be executed at specific stages during training.\n",
      "     |      shrink_windows : bool, optional\n",
      "     |          New in 4.1. Experimental.\n",
      "     |          If True, the effective window size is uniformly sampled from  [1, `window`]\n",
      "     |          for each target word during training, to match the original word2vec algorithm's\n",
      "     |          approximate weighting of context words by distance. Otherwise, the effective\n",
      "     |          window size is always fixed to `window` words to either side.\n",
      "     |\n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
      "     |\n",
      "     |      .. sourcecode:: pycon\n",
      "     |\n",
      "     |          >>> from gensim.models import Word2Vec\n",
      "     |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      "     |          >>> model = Word2Vec(sentences, min_count=1)\n",
      "     |\n",
      "     |      Attributes\n",
      "     |      ----------\n",
      "     |      wv : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      "     |          This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      "     |          directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      "     |\n",
      "     |  __str__(self)\n",
      "     |      Human readable representation of the model's state.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      str\n",
      "     |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      "     |          and learning rate.\n",
      "     |\n",
      "     |  add_null_word(self)\n",
      "     |\n",
      "     |  build_vocab(self, corpus_iterable=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      "     |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      corpus_iterable : iterable of list of str\n",
      "     |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      "     |          `corpus_file` arguments need to be passed (not both of them).\n",
      "     |      update : bool\n",
      "     |          If true, the new words in `sentences` will be added to model's vocab.\n",
      "     |      progress_per : int, optional\n",
      "     |          Indicates how many words to process before showing/updating the progress.\n",
      "     |      keep_raw_vocab : bool, optional\n",
      "     |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      "     |          of the model.\n",
      "     |\n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |\n",
      "     |      **kwargs : object\n",
      "     |          Keyword arguments propagated to `self.prepare_vocab`.\n",
      "     |\n",
      "     |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      "     |      Build vocabulary from a dictionary of word frequencies.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      word_freq : dict of (str, int)\n",
      "     |          A mapping from a word in the vocabulary to its frequency count.\n",
      "     |      keep_raw_vocab : bool, optional\n",
      "     |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      "     |      corpus_count : int, optional\n",
      "     |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      "     |      trim_rule : function, optional\n",
      "     |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "     |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "     |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "     |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "     |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "     |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      "     |          of the model.\n",
      "     |\n",
      "     |          The input parameters are of the following types:\n",
      "     |              * `word` (str) - the word we are examining\n",
      "     |              * `count` (int) - the word's frequency count in the corpus\n",
      "     |              * `min_count` (int) - the minimum count threshold.\n",
      "     |\n",
      "     |      update : bool, optional\n",
      "     |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      "     |\n",
      "     |  create_binary_tree(self)\n",
      "     |      Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n",
      "     |      word counts. Frequent words will have shorter binary codes.\n",
      "     |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      "     |\n",
      "     |  estimate_memory(self, vocab_size=None, report=None)\n",
      "     |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      vocab_size : int, optional\n",
      "     |          Number of unique tokens in the vocabulary\n",
      "     |      report : dict of (str, int), optional\n",
      "     |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dict of (str, int)\n",
      "     |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      "     |\n",
      "     |  get_latest_training_loss(self)\n",
      "     |      Get current value of the training loss.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      float\n",
      "     |          Current training loss.\n",
      "     |\n",
      "     |  init_sims(self, replace=False)\n",
      "     |      Precompute L2-normalized vectors. Obsoleted.\n",
      "     |\n",
      "     |      If you need a single unit-normalized vector for some key, call\n",
      "     |      :meth:`~gensim.models.keyedvectors.KeyedVectors.get_vector` instead:\n",
      "     |      ``word2vec_model.wv.get_vector(key, norm=True)``.\n",
      "     |\n",
      "     |      To refresh norms after you performed some atypical out-of-band vector tampering,\n",
      "     |      call `:meth:`~gensim.models.keyedvectors.KeyedVectors.fill_norms()` instead.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      replace : bool\n",
      "     |          If True, forget the original trained vectors and only keep the normalized ones.\n",
      "     |          You lose information if you do this.\n",
      "     |\n",
      "     |  init_weights(self)\n",
      "     |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
      "     |\n",
      "     |  make_cum_table(self, domain=2147483647)\n",
      "     |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
      "     |      drawing random words in the negative-sampling training routines.\n",
      "     |\n",
      "     |      To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n",
      "     |      then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n",
      "     |      That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n",
      "     |\n",
      "     |  predict_output_word(self, context_words_list, topn=10)\n",
      "     |      Get the probability distribution of the center word given context words.\n",
      "     |\n",
      "     |      Note this performs a CBOW-style propagation, even in SG models,\n",
      "     |      and doesn't quite weight the surrounding words the same as in\n",
      "     |      training -- so it's just one crude way of using a trained model\n",
      "     |      as a predictor.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      context_words_list : list of (str and/or int)\n",
      "     |          List of context words, which may be words themselves (str)\n",
      "     |          or their index in `self.wv.vectors` (int).\n",
      "     |      topn : int, optional\n",
      "     |          Return `topn` words and their probabilities.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      list of (str, float)\n",
      "     |          `topn` length list of tuples of (word, probability).\n",
      "     |\n",
      "     |  prepare_vocab(self, update=False, keep_raw_vocab=False, trim_rule=None, min_count=None, sample=None, dry_run=False)\n",
      "     |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
      "     |      and `sample` (controlling the downsampling of more-frequent words).\n",
      "     |\n",
      "     |      Calling with `dry_run=True` will only simulate the provided settings and\n",
      "     |      report the size of the retained vocabulary, effective corpus length, and\n",
      "     |      estimated memory requirements. Results are both printed via logging and\n",
      "     |      returned as a dict.\n",
      "     |\n",
      "     |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
      "     |      unless `keep_raw_vocab` is set.\n",
      "     |\n",
      "     |  prepare_weights(self, update=False)\n",
      "     |      Build tables and model weights based on final vocabulary settings.\n",
      "     |\n",
      "     |  reset_from(self, other_model)\n",
      "     |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      "     |\n",
      "     |      Structures copied are:\n",
      "     |          * Vocabulary\n",
      "     |          * Index to word mapping\n",
      "     |          * Cumulative frequency table (used for negative sampling)\n",
      "     |          * Cached corpus length\n",
      "     |\n",
      "     |      Useful when testing multiple models on the same corpus in parallel. However, as the models\n",
      "     |      then share all vocabulary-related structures other than vectors, neither should then\n",
      "     |      expand their vocabulary (which could leave the other in an inconsistent, broken state).\n",
      "     |      And, any changes to any per-word 'vecattr' will affect both models.\n",
      "     |\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "     |          Another model to copy the internal structures from.\n",
      "     |\n",
      "     |  save(self, *args, **kwargs)\n",
      "     |      Save the model.\n",
      "     |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
      "     |      online training and getting vectors for vocabulary words.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to the file.\n",
      "     |\n",
      "     |  scan_vocab(self, corpus_iterable=None, corpus_file=None, progress_per=10000, workers=None, trim_rule=None)\n",
      "     |\n",
      "     |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      "     |      Score the log probability for a sequence of sentences.\n",
      "     |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      "     |\n",
      "     |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      "     |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      "     |\n",
      "     |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      "     |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      "     |\n",
      "     |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      "     |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      "     |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      "     |      how to use such scores in document classification.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sentences : iterable of list of str\n",
      "     |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      "     |      total_sentences : int, optional\n",
      "     |          Count of sentences.\n",
      "     |      chunksize : int, optional\n",
      "     |          Chunksize of jobs\n",
      "     |      queue_factor : int, optional\n",
      "     |          Multiplier for size of queue (number of workers * queue_factor).\n",
      "     |      report_delay : float, optional\n",
      "     |          Seconds to wait before reporting progress.\n",
      "     |\n",
      "     |  seeded_vector(self, seed_string, vector_size)\n",
      "     |\n",
      "     |  train(self, corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs)\n",
      "     |      Update the model's neural weights from a sequence of sentences.\n",
      "     |\n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      "     |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      "     |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      "     |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      "     |      you can simply use `total_examples=self.corpus_count`.\n",
      "     |\n",
      "     |      Warnings\n",
      "     |      --------\n",
      "     |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      "     |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      "     |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.epochs`.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      corpus_iterable : iterable of list of str\n",
      "     |          The ``corpus_iterable`` can be simply a list of lists of tokens, but for larger corpora,\n",
      "     |          consider an iterable that streams the sentences directly from disk/network, to limit RAM usage.\n",
      "     |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "     |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      "     |          See also the `tutorial on data streaming in Python\n",
      "     |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      "     |      corpus_file : str, optional\n",
      "     |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "     |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      "     |          `corpus_file` arguments need to be passed (not both of them).\n",
      "     |      total_examples : int\n",
      "     |          Count of sentences.\n",
      "     |      total_words : int\n",
      "     |          Count of raw words in sentences.\n",
      "     |      epochs : int\n",
      "     |          Number of iterations (epochs) over the corpus.\n",
      "     |      start_alpha : float, optional\n",
      "     |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      "     |          for this one call to`train()`.\n",
      "     |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      "     |          (not recommended).\n",
      "     |      end_alpha : float, optional\n",
      "     |          Final learning rate. Drops linearly from `start_alpha`.\n",
      "     |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      "     |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      "     |          (not recommended).\n",
      "     |      word_count : int, optional\n",
      "     |          Count of words already trained. Set this to 0 for the usual\n",
      "     |          case of training on all words in sentences.\n",
      "     |      queue_factor : int, optional\n",
      "     |          Multiplier for size of queue (number of workers * queue_factor).\n",
      "     |      report_delay : float, optional\n",
      "     |          Seconds to wait before reporting progress.\n",
      "     |      compute_loss: bool, optional\n",
      "     |          If True, computes and stores loss value which can be retrieved using\n",
      "     |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      "     |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      "     |          Sequence of callbacks to be executed at specific stages during training.\n",
      "     |\n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      .. sourcecode:: pycon\n",
      "     |\n",
      "     |          >>> from gensim.models import Word2Vec\n",
      "     |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      "     |          >>>\n",
      "     |          >>> model = Word2Vec(min_count=1)\n",
      "     |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      "     |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n",
      "     |          (1, 30)\n",
      "     |\n",
      "     |  update_weights(self)\n",
      "     |      Copy all the existing weights, and reset the weights for the newly added vocabulary.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |\n",
      "     |  load(*args, rethrow=False, **kwargs)\n",
      "     |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
      "     |\n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
      "     |          Save model.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to the saved file.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`~gensim.models.word2vec.Word2Vec`\n",
      "     |          Loaded model.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |\n",
      "     |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      "     |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      "     |      optionally log the event at `log_level`.\n",
      "     |\n",
      "     |      Events are important moments during the object's life, such as \"model created\",\n",
      "     |      \"model saved\", \"model loaded\", etc.\n",
      "     |\n",
      "     |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      "     |      but is useful during debugging and support.\n",
      "     |\n",
      "     |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      "     |      will not record events into `self.lifecycle_events` then.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      event_name : str\n",
      "     |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      "     |      event : dict\n",
      "     |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      "     |          Can be empty.\n",
      "     |\n",
      "     |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      "     |\n",
      "     |          - `datetime`: the current date & time\n",
      "     |          - `gensim`: the current Gensim version\n",
      "     |          - `python`: the current Python version\n",
      "     |          - `platform`: the current platform\n",
      "     |          - `event`: the name of this event\n",
      "     |      log_level : int\n",
      "     |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "    class Word2VecTrainables(gensim.utils.SaveLoad)\n",
      "     |  Obsolete class retained for now as load-compatibility state capture.\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      Word2VecTrainables\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |\n",
      "     |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      "     |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      "     |      optionally log the event at `log_level`.\n",
      "     |\n",
      "     |      Events are important moments during the object's life, such as \"model created\",\n",
      "     |      \"model saved\", \"model loaded\", etc.\n",
      "     |\n",
      "     |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      "     |      but is useful during debugging and support.\n",
      "     |\n",
      "     |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      "     |      will not record events into `self.lifecycle_events` then.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      event_name : str\n",
      "     |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      "     |      event : dict\n",
      "     |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      "     |          Can be empty.\n",
      "     |\n",
      "     |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      "     |\n",
      "     |          - `datetime`: the current date & time\n",
      "     |          - `gensim`: the current Gensim version\n",
      "     |          - `python`: the current Python version\n",
      "     |          - `platform`: the current platform\n",
      "     |          - `event`: the name of this event\n",
      "     |      log_level : int\n",
      "     |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      "     |\n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=4)\n",
      "     |      Save the object to a file.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname_or_handle : str or file-like\n",
      "     |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      "     |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      "     |      separately : list of str or None, optional\n",
      "     |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      "     |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      "     |          loading and sharing the large arrays in RAM between multiple processes.\n",
      "     |\n",
      "     |          If list of str: store these attributes into separate files. The automated size check\n",
      "     |          is not performed in this case.\n",
      "     |      sep_limit : int, optional\n",
      "     |          Don't store arrays smaller than this separately. In bytes.\n",
      "     |      ignore : frozenset of str, optional\n",
      "     |          Attributes that shouldn't be stored at all.\n",
      "     |      pickle_protocol : int, optional\n",
      "     |          Protocol number for pickle.\n",
      "     |\n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.load`\n",
      "     |          Load object from file.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |\n",
      "     |  load(fname, mmap=None)\n",
      "     |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to file that contains needed object.\n",
      "     |      mmap : str, optional\n",
      "     |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      "     |          via mmap (shared memory) using `mmap='r'.\n",
      "     |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      "     |\n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |          Save object to file.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      object\n",
      "     |          Object loaded from `fname`.\n",
      "     |\n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          When called on an object instance instead of class (this is a class method).\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "    class Word2VecVocab(gensim.utils.SaveLoad)\n",
      "     |  Obsolete class retained for now as load-compatibility state capture.\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      Word2VecVocab\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |\n",
      "     |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      "     |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      "     |      optionally log the event at `log_level`.\n",
      "     |\n",
      "     |      Events are important moments during the object's life, such as \"model created\",\n",
      "     |      \"model saved\", \"model loaded\", etc.\n",
      "     |\n",
      "     |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      "     |      but is useful during debugging and support.\n",
      "     |\n",
      "     |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      "     |      will not record events into `self.lifecycle_events` then.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      event_name : str\n",
      "     |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      "     |      event : dict\n",
      "     |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      "     |          Can be empty.\n",
      "     |\n",
      "     |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      "     |\n",
      "     |          - `datetime`: the current date & time\n",
      "     |          - `gensim`: the current Gensim version\n",
      "     |          - `python`: the current Python version\n",
      "     |          - `platform`: the current platform\n",
      "     |          - `event`: the name of this event\n",
      "     |      log_level : int\n",
      "     |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      "     |\n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=4)\n",
      "     |      Save the object to a file.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname_or_handle : str or file-like\n",
      "     |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      "     |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      "     |      separately : list of str or None, optional\n",
      "     |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      "     |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      "     |          loading and sharing the large arrays in RAM between multiple processes.\n",
      "     |\n",
      "     |          If list of str: store these attributes into separate files. The automated size check\n",
      "     |          is not performed in this case.\n",
      "     |      sep_limit : int, optional\n",
      "     |          Don't store arrays smaller than this separately. In bytes.\n",
      "     |      ignore : frozenset of str, optional\n",
      "     |          Attributes that shouldn't be stored at all.\n",
      "     |      pickle_protocol : int, optional\n",
      "     |          Protocol number for pickle.\n",
      "     |\n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.load`\n",
      "     |          Load object from file.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |\n",
      "     |  load(fname, mmap=None)\n",
      "     |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      "     |\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : str\n",
      "     |          Path to file that contains needed object.\n",
      "     |      mmap : str, optional\n",
      "     |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      "     |          via mmap (shared memory) using `mmap='r'.\n",
      "     |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      "     |\n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      :meth:`~gensim.utils.SaveLoad.save`\n",
      "     |          Save object to file.\n",
      "     |\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      object\n",
      "     |          Object loaded from `fname`.\n",
      "     |\n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          When called on an object instance instead of class (this is a class method).\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "FUNCTIONS\n",
      "    default_timer = perf_counter(...)\n",
      "        perf_counter() -> float\n",
      "\n",
      "        Performance counter for benchmarking.\n",
      "\n",
      "    score_sentence_cbow(...)\n",
      "        score_sentence_cbow(model, sentence, _work, _neu1)\n",
      "        Obtain likelihood score for a single sentence in a fitted CBOW representation.\n",
      "\n",
      "            Notes\n",
      "            -----\n",
      "            This scoring function is only implemented for hierarchical softmax (`model.hs == 1`).\n",
      "            The model should have been trained using the skip-gram model (`model.cbow` == 1`).\n",
      "\n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The trained model. It **MUST** have been trained using hierarchical softmax and the CBOW algorithm.\n",
      "            sentence : list of str\n",
      "                The words comprising the sentence to be scored.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            _neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "\n",
      "            Returns\n",
      "            -------\n",
      "            float\n",
      "                The probability assigned to this sentence by the Skip-Gram model.\n",
      "\n",
      "    score_sentence_sg(...)\n",
      "        score_sentence_sg(model, sentence, _work)\n",
      "        Obtain likelihood score for a single sentence in a fitted skip-gram representation.\n",
      "\n",
      "            Notes\n",
      "            -----\n",
      "            This scoring function is only implemented for hierarchical softmax (`model.hs == 1`).\n",
      "            The model should have been trained using the skip-gram model (`model.sg` == 1`).\n",
      "\n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The trained model. It **MUST** have been trained using hierarchical softmax and the skip-gram algorithm.\n",
      "            sentence : list of str\n",
      "                The words comprising the sentence to be scored.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "\n",
      "            Returns\n",
      "            -------\n",
      "            float\n",
      "                The probability assigned to this sentence by the Skip-Gram model.\n",
      "\n",
      "    train_batch_cbow(...)\n",
      "        train_batch_cbow(model, sentences, alpha, _work, _neu1, compute_loss)\n",
      "        Update CBOW model by training on a batch of sentences.\n",
      "\n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "\n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            sentences : iterable of list of str\n",
      "                The corpus used to train the model.\n",
      "            alpha : float\n",
      "                The learning rate.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            _neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "\n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "\n",
      "    train_batch_sg(...)\n",
      "        train_batch_sg(model, sentences, alpha, _work, compute_loss)\n",
      "        Update skip-gram model by training on a batch of sentences.\n",
      "\n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "\n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2Vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            sentences : iterable of list of str\n",
      "                The corpus used to train the model.\n",
      "            alpha : float\n",
      "                The learning rate\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "\n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "\n",
      "    train_epoch_cbow(...)\n",
      "        train_epoch_cbow(model, corpus_file, offset, _cython_vocab, _cur_epoch, _expected_examples, _expected_words, _work, _neu1, compute_loss)\n",
      "        Train CBOW model for one epoch by training on an input stream. This function is used only in multistream mode.\n",
      "\n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "\n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            corpus_file : str\n",
      "                Path to corpus file.\n",
      "            _cur_epoch : int\n",
      "                Current epoch number. Used for calculating and decaying learning rate.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            _neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "\n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "\n",
      "    train_epoch_sg(...)\n",
      "        train_epoch_sg(model, corpus_file, offset, _cython_vocab, _cur_epoch, _expected_examples, _expected_words, _work, _neu1, compute_loss)\n",
      "        Train Skipgram model for one epoch by training on an input stream. This function is used only in multistream mode.\n",
      "\n",
      "            Called internally from :meth:`~gensim.models.word2vec.Word2Vec.train`.\n",
      "\n",
      "            Parameters\n",
      "            ----------\n",
      "            model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      "                The Word2Vec model instance to train.\n",
      "            corpus_file : str\n",
      "                Path to corpus file.\n",
      "            _cur_epoch : int\n",
      "                Current epoch number. Used for calculating and decaying learning rate.\n",
      "            _work : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            _neu1 : np.ndarray\n",
      "                Private working memory for each worker.\n",
      "            compute_loss : bool\n",
      "                Whether or not the training loss should be computed in this batch.\n",
      "\n",
      "            Returns\n",
      "            -------\n",
      "            int\n",
      "                Number of words in the vocabulary actually used for training (They already existed in the vocabulary\n",
      "                and were not discarded by negative sampling).\n",
      "\n",
      "DATA\n",
      "    CORPUSFILE_VERSION = 1\n",
      "    FAST_VERSION = 0\n",
      "    MAX_WORDS_IN_BATCH = 10000\n",
      "    logger = <Logger gensim.models.word2vec (WARNING)>\n",
      "\n",
      "FILE\n",
      "    c:\\users\\quent\\documents\\master\\1er\\nlp\\pythonproject\\.venv\\lib\\site-packages\\gensim\\models\\word2vec.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "id": "working-titanium",
   "metadata": {},
   "source": [
    "**1b.** Please download from Gensim the `word2vec-google-news-300` model, upon your first use.  Then, please write code to answer the following questions:\n",
    "* Where is the model stored on your computer and what is the file name?  You can store the absolute path in a variable called `path_to_model_file`.\n",
    "* What is the size of the corresponding file?  Please display the size in gigabytes with two decimals."
   ]
  },
  {
   "cell_type": "code",
   "id": "infectious-burner",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T10:58:23.398104Z",
     "start_time": "2025-02-27T10:57:58.325074Z"
    }
   },
   "source": [
    "# Download the model from Gensim (needed only the first time)\n",
    "gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "# No need to store the returned value (uses a lot of memory)."
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[64], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Download the model from Gensim (needed only the first time)\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mgensim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownloader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mword2vec-google-news-300\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# No need to store the returned value (uses a lot of memory).\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\master\\1er\\nlp\\pythonProject\\.venv\\Lib\\site-packages\\gensim\\downloader.py:503\u001B[0m, in \u001B[0;36mload\u001B[1;34m(name, return_path)\u001B[0m\n\u001B[0;32m    501\u001B[0m sys\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39minsert(\u001B[38;5;241m0\u001B[39m, BASE_DIR)\n\u001B[0;32m    502\u001B[0m module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28m__import__\u001B[39m(name)\n\u001B[1;32m--> 503\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~/gensim-data\\word2vec-google-news-300\\__init__.py:8\u001B[0m, in \u001B[0;36mload_data\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mload_data\u001B[39m():\n\u001B[0;32m      7\u001B[0m     path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(base_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mword2vec-google-news-300\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mword2vec-google-news-300.gz\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mKeyedVectors\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_word2vec_format\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbinary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[1;32m~\\Documents\\master\\1er\\nlp\\pythonProject\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001B[0m, in \u001B[0;36mKeyedVectors.load_word2vec_format\u001B[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001B[0m\n\u001B[0;32m   1672\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m   1673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mload_word2vec_format\u001B[39m(\n\u001B[0;32m   1674\u001B[0m         \u001B[38;5;28mcls\u001B[39m, fname, fvocab\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf8\u001B[39m\u001B[38;5;124m'\u001B[39m, unicode_errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstrict\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   1675\u001B[0m         limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, datatype\u001B[38;5;241m=\u001B[39mREAL, no_header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   1676\u001B[0m     ):\n\u001B[0;32m   1677\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001B[39;00m\n\u001B[0;32m   1678\u001B[0m \n\u001B[0;32m   1679\u001B[0m \u001B[38;5;124;03m    Warnings\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1717\u001B[0m \n\u001B[0;32m   1718\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1719\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load_word2vec_format\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1720\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfvocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbinary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbinary\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43municode_errors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43municode_errors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1721\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatatype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdatatype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mno_header\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mno_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1722\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\master\\1er\\nlp\\pythonProject\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:2065\u001B[0m, in \u001B[0;36m_load_word2vec_format\u001B[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001B[0m\n\u001B[0;32m   2062\u001B[0m kv \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(vector_size, vocab_size, dtype\u001B[38;5;241m=\u001B[39mdatatype)\n\u001B[0;32m   2064\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m-> 2065\u001B[0m     \u001B[43m_word2vec_read_binary\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2066\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfin\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcounts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocab_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvector_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatatype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43municode_errors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbinary_chunk_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\n\u001B[0;32m   2067\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2068\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2069\u001B[0m     _word2vec_read_text(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\n",
      "File \u001B[1;32m~\\Documents\\master\\1er\\nlp\\pythonProject\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1960\u001B[0m, in \u001B[0;36m_word2vec_read_binary\u001B[1;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding)\u001B[0m\n\u001B[0;32m   1958\u001B[0m new_chunk \u001B[38;5;241m=\u001B[39m fin\u001B[38;5;241m.\u001B[39mread(binary_chunk_size)\n\u001B[0;32m   1959\u001B[0m chunk \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m new_chunk\n\u001B[1;32m-> 1960\u001B[0m processed_words, chunk \u001B[38;5;241m=\u001B[39m \u001B[43m_add_bytes_to_kv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1961\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcounts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocab_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvector_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatatype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43municode_errors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1962\u001B[0m tot_processed_words \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m processed_words\n\u001B[0;32m   1963\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(new_chunk) \u001B[38;5;241m<\u001B[39m binary_chunk_size:\n",
      "File \u001B[1;32m~\\Documents\\master\\1er\\nlp\\pythonProject\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1943\u001B[0m, in \u001B[0;36m_add_bytes_to_kv\u001B[1;34m(kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001B[0m\n\u001B[0;32m   1941\u001B[0m word \u001B[38;5;241m=\u001B[39m word\u001B[38;5;241m.\u001B[39mlstrip(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m   1942\u001B[0m vector \u001B[38;5;241m=\u001B[39m frombuffer(chunk, offset\u001B[38;5;241m=\u001B[39mi_vector, count\u001B[38;5;241m=\u001B[39mvector_size, dtype\u001B[38;5;241m=\u001B[39mREAL)\u001B[38;5;241m.\u001B[39mastype(datatype)\n\u001B[1;32m-> 1943\u001B[0m \u001B[43m_add_word_to_kv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcounts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mword\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocab_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1944\u001B[0m start \u001B[38;5;241m=\u001B[39m i_vector \u001B[38;5;241m+\u001B[39m bytes_per_vector\n\u001B[0;32m   1945\u001B[0m processed_words \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\Documents\\master\\1er\\nlp\\pythonProject\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1923\u001B[0m, in \u001B[0;36m_add_word_to_kv\u001B[1;34m(kv, counts, word, weights, vocab_size)\u001B[0m\n\u001B[0;32m   1921\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvocabulary file is incomplete: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is missing\u001B[39m\u001B[38;5;124m\"\u001B[39m, word)\n\u001B[0;32m   1922\u001B[0m     word_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1923\u001B[0m \u001B[43mkv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_vecattr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcount\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mword_count\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\master\\1er\\nlp\\pythonProject\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:334\u001B[0m, in \u001B[0;36mKeyedVectors.set_vecattr\u001B[1;34m(self, key, attr, val)\u001B[0m\n\u001B[0;32m    331\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexpandos[attr] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(target_size, dtype\u001B[38;5;241m=\u001B[39mprev_expando\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexpandos[attr][: \u001B[38;5;28mmin\u001B[39m(prev_count, target_size), ] \u001B[38;5;241m=\u001B[39m prev_expando[: \u001B[38;5;28mmin\u001B[39m(prev_count, target_size), ]\n\u001B[1;32m--> 334\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mset_vecattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, key, attr, val):\n\u001B[0;32m    335\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Set attribute associated with the given key to value.\u001B[39;00m\n\u001B[0;32m    336\u001B[0m \n\u001B[0;32m    337\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    351\u001B[0m \n\u001B[0;32m    352\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mallocate_vecattrs(attrs\u001B[38;5;241m=\u001B[39m[attr], types\u001B[38;5;241m=\u001B[39m[\u001B[38;5;28mtype\u001B[39m(val)])\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "id": "scheduled-binary",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T09:41:02.672379Z",
     "start_time": "2025-02-27T09:41:02.660855Z"
    }
   },
   "source": [
    "# Please write your Python code below and execute it.\n",
    "path_to_model_file=\"C:/Users/quent/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\"\n",
    "print(path_to_model_file)\n",
    "print(round(os.path.getsize(path_to_model_file)/(1024**3),2))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/quent/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "1.62\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "western-insurance",
   "metadata": {},
   "source": [
    "**1c.** Please load the word2vec model as an instance of the class `KeyedVectors`, and store it in a variable called `wv_model`. \n",
    "What is, at this point, the memory size of the process corresponding to this notebook?  Simply write the value you obtain from any OS-specific utility that you like."
   ]
  },
  {
   "cell_type": "code",
   "id": "instant-jewelry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T10:59:27.892598Z",
     "start_time": "2025-02-27T10:58:56.995671Z"
    }
   },
   "source": [
    "# Please write your Python code below and execute it.  Write the memory size on a commented line.\n",
    "wv_model = KeyedVectors.load_word2vec_format(path_to_model_file, binary=True)\n",
    "# process memory usage : 3.5GB"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "id": "together-flooring",
   "metadata": {},
   "source": [
    "**1d.** Please write the instructions that generate the answers to the following questions.\n",
    "* What is the size of the vocabulary of the `wv_model` model?  \n",
    "* What is the dimensionality of each word vector?  \n",
    "* What is the word corresponding to the vector in position 1234?  \n",
    "* What are the first 10 coefficients of the word vector for the word *pyramid*?  "
   ]
  },
  {
   "cell_type": "code",
   "id": "80d86cc1a7715bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T11:01:26.486636Z",
     "start_time": "2025-02-27T11:01:24.791349Z"
    }
   },
   "source": [
    "# Please write your Python code below and execute it.\n",
    "# Please write your Python code below and execute it.\n",
    "print(\"vocab size\",wv_model.vectors.shape[0])\n",
    "print(\"size of word vector\",wv_model.vectors.shape[1])\n",
    "print(wv_model.index_to_key[1234])\n",
    "print(wv_model.most_similar(\"pyramid\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 3000000\n",
      "size of word vector 300\n",
      "learn\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[68], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msize of word vector\u001B[39m\u001B[38;5;124m\"\u001B[39m,wv_model\u001B[38;5;241m.\u001B[39mvectors\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(wv_model\u001B[38;5;241m.\u001B[39mindex_to_key[\u001B[38;5;241m1234\u001B[39m])\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mwv_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmost_similar\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpyramid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\Documents\\master\\1er\\nlp\\pythonProject\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:823\u001B[0m, in \u001B[0;36mKeyedVectors.most_similar\u001B[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001B[0m\n\u001B[0;32m    820\u001B[0m positive \u001B[38;5;241m=\u001B[39m _ensure_list(positive)\n\u001B[0;32m    821\u001B[0m negative \u001B[38;5;241m=\u001B[39m _ensure_list(negative)\n\u001B[1;32m--> 823\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfill_norms\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    824\u001B[0m clip_end \u001B[38;5;241m=\u001B[39m clip_end \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvectors)\n\u001B[0;32m    826\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m restrict_vocab:\n",
      "File \u001B[1;32m~\\Documents\\master\\1er\\nlp\\pythonProject\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:708\u001B[0m, in \u001B[0;36mKeyedVectors.fill_norms\u001B[1;34m(self, force)\u001B[0m\n\u001B[0;32m    700\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    701\u001B[0m \u001B[38;5;124;03mEnsure per-vector norms are available.\u001B[39;00m\n\u001B[0;32m    702\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    705\u001B[0m \n\u001B[0;32m    706\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    707\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorms \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m force:\n\u001B[1;32m--> 708\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorms \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinalg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvectors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\master\\1er\\nlp\\pythonProject\\.venv\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2582\u001B[0m, in \u001B[0;36mnorm\u001B[1;34m(x, ord, axis, keepdims)\u001B[0m\n\u001B[0;32m   2579\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m add\u001B[38;5;241m.\u001B[39mreduce(\u001B[38;5;28mabs\u001B[39m(x), axis\u001B[38;5;241m=\u001B[39maxis, keepdims\u001B[38;5;241m=\u001B[39mkeepdims)\n\u001B[0;32m   2580\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mord\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mord\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m   2581\u001B[0m     \u001B[38;5;66;03m# special case for speedup\u001B[39;00m\n\u001B[1;32m-> 2582\u001B[0m     s \u001B[38;5;241m=\u001B[39m (\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconj\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m)\u001B[38;5;241m.\u001B[39mreal\n\u001B[0;32m   2583\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m sqrt(add\u001B[38;5;241m.\u001B[39mreduce(s, axis\u001B[38;5;241m=\u001B[39maxis, keepdims\u001B[38;5;241m=\u001B[39mkeepdims))\n\u001B[0;32m   2584\u001B[0m \u001B[38;5;66;03m# None of the str-type keywords for ord ('fro', 'nuc')\u001B[39;00m\n\u001B[0;32m   2585\u001B[0m \u001B[38;5;66;03m# are valid for vectors\u001B[39;00m\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "id": "governing-accessory",
   "metadata": {},
   "source": [
    "## 2. Solving analogies using word2vec trained on Google News\n",
    "In this section, you are going to use word vectors to solve analogy tasks provided with Gensim, such as \"What is to France what Rome is to Italy?\".  The predefined function in Gensim that evaluates a model on this task does not provide enough details, so you will need to make modifications to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642353b",
   "metadata": {},
   "source": [
    "**2a.** The analogy tasks are stored in a text file called `questions-words.txt` which is typically found in `C:\\Users\\YourNameHere\\.conda\\envs\\YourEnvNameHere\\Lib\\site-packages\\gensim\\test\\test_data`.  You can access it from here with Gensim as `datapath('questions-words.txt')`.  \n",
    "\n",
    "Please create a file called `questions-words-100.txt` with the first 100 lines from the original file.  Please run the evaluation task on this file, using the [documentation of the KeyedVectors class](https://radimrehurek.com/gensim/models/keyedvectors.html), then answer the following questions:\n",
    "* How many analogy tasks are there in your `questions-words-100.txt` file?\n",
    "* How many analogies were solved correctly and how many incorrectly?\n",
    "* What is the accuracy returned by `evaluate_word_analogies`?\n",
    "* How much time did it take to solve the analogies?"
   ]
  },
  {
   "cell_type": "code",
   "id": "1ae43e45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T10:29:10.940807Z",
     "start_time": "2025-02-27T10:29:08.620289Z"
    }
   },
   "source": [
    "# Please write your Python code below and execute it.\n",
    "directory =\"C:/Users/quent/Documents/master/1er/nlp/pythonProject/.venv/Lib/site-packages/gensim/test/test_data\"\n",
    "output = directory+\"/questions-words-100.txt\"\n",
    "input = directory+\"/questions-words.txt\"\n",
    "if not os.path.exists(directory+\"/\"+\"questions-words-100.txt\"\"questions-words-100.txt\"):\n",
    "    with open(input, \"r\") as infile, open(output, \"w\") as outfile:\n",
    "       outfile.writelines(itertools.islice(infile, 100))\n"
   ],
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'Patrick_Nyarko' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m infile, \u001B[38;5;28mopen\u001B[39m(output, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m outfile:\n\u001B[0;32m      7\u001B[0m        outfile\u001B[38;5;241m.\u001B[39mwritelines(itertools\u001B[38;5;241m.\u001B[39mislice(infile, \u001B[38;5;241m100\u001B[39m))\n\u001B[1;32m----> 8\u001B[0m analogy_scores \u001B[38;5;241m=\u001B[39m\u001B[43mwv_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate_word_analogies\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdatapath\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mquestions-words-100.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\master\\1er\\nlp\\pythonProject\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1328\u001B[0m, in \u001B[0;36mKeyedVectors.evaluate_word_analogies\u001B[1;34m(self, analogies, restrict_vocab, case_insensitive, dummy4unknown, similarity_function)\u001B[0m\n\u001B[0;32m   1326\u001B[0m ok_keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex_to_key[:restrict_vocab]\n\u001B[0;32m   1327\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m case_insensitive:\n\u001B[1;32m-> 1328\u001B[0m     ok_vocab \u001B[38;5;241m=\u001B[39m {k\u001B[38;5;241m.\u001B[39mupper(): \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(ok_keys)}\n\u001B[0;32m   1329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1330\u001B[0m     ok_vocab \u001B[38;5;241m=\u001B[39m {k: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_index(k) \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(ok_keys)}\n",
      "File \u001B[1;32m~\\Documents\\master\\1er\\nlp\\pythonProject\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:420\u001B[0m, in \u001B[0;36mKeyedVectors.get_index\u001B[1;34m(self, key, default)\u001B[0m\n\u001B[0;32m    418\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m default\n\u001B[0;32m    419\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 420\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mKey \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m not present\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: \"Key 'Patrick_Nyarko' not present\""
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T10:57:36.930516Z",
     "start_time": "2025-02-27T10:57:36.874167Z"
    }
   },
   "cell_type": "code",
   "source": "analogy_scores = (wv_model.evaluate_word_analogies(datapath('questions-words.txt'),dummy4unknown=True))\n",
   "id": "86ea85486312e4fc",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'Patrick_Nyarko' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[62], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m analogy_scores \u001B[38;5;241m=\u001B[39m (\u001B[43mwv_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate_word_analogies\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdatapath\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mquestions-words.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdummy4unknown\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\Documents\\master\\1er\\nlp\\pythonProject\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1328\u001B[0m, in \u001B[0;36mKeyedVectors.evaluate_word_analogies\u001B[1;34m(self, analogies, restrict_vocab, case_insensitive, dummy4unknown, similarity_function)\u001B[0m\n\u001B[0;32m   1326\u001B[0m ok_keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex_to_key[:restrict_vocab]\n\u001B[0;32m   1327\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m case_insensitive:\n\u001B[1;32m-> 1328\u001B[0m     ok_vocab \u001B[38;5;241m=\u001B[39m {k\u001B[38;5;241m.\u001B[39mupper(): \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(ok_keys)}\n\u001B[0;32m   1329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1330\u001B[0m     ok_vocab \u001B[38;5;241m=\u001B[39m {k: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_index(k) \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(ok_keys)}\n",
      "File \u001B[1;32m~\\Documents\\master\\1er\\nlp\\pythonProject\\.venv\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:420\u001B[0m, in \u001B[0;36mKeyedVectors.get_index\u001B[1;34m(self, key, default)\u001B[0m\n\u001B[0;32m    418\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m default\n\u001B[0;32m    419\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 420\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mKey \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m not present\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: \"Key 'Patrick_Nyarko' not present\""
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "markdown",
   "id": "b8da425e",
   "metadata": {},
   "source": [
    "**2b.** Please answer in writing the following questions:\n",
    "* What is the meaning of the first line of `questions-words-100.txt`?\n",
    "* How many analogies are there in the original `questions-words.txt`?\n",
    "* How much time would it take to solve the original set of analogies?"
   ]
  },
  {
   "cell_type": "code",
   "id": "66241a78",
   "metadata": {},
   "source": [
    "# Please write your answers here.\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "683ea5d4",
   "metadata": {},
   "source": [
    "**2c.** The built-in function from Gensim has several weaknesses, which you will address here.  Please copy the source code of the function `evaluate_word_analogies` from the file `gensim\\models\\keyedvectors.py` and create here a new function which will improve the built-in one as follows.  The function will be called `my_evaluate_word_analogies` and you will also pass it the model as the first argument.  Overall, please proceed gradually and only make minimal modifications, to ensure you don't break the function.  It is important to first understand the structure of the result, `analogies_scores` and `sections`. \n",
    "\n",
    "* Modify the line where `section[incorrect]` is assembled in order to also add to each analogy the *incorrect guess* (i.e. what the model thought was the good answer, but got it wrong).\n",
    "\n",
    "* Modify the code so that when `section[incorrect]` is assembled, you also add the *rank of the correct answer* among the candidates returned by the system (after the incorrect guess).  If the correct answer is not present at all, then code the rank as 0."
   ]
  },
  {
   "cell_type": "code",
   "id": "e054d8ad",
   "metadata": {},
   "source": [
    "def my_evaluate_word_analogies(model, analogies, restrict_vocab=300000, case_insensitive=True):\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "062fec19",
   "metadata": {},
   "source": [
    "**2d.** Please run the `my_evaluate_word_analogies` function on `questions-words-100.txt` and then write instructions to display, from the results stored in `analogy_scores`:\n",
    "* one incorrectly-solved analogy (selected at random), including also the error made by the model and the rank of the correct answer, thus adding:\n",
    "  - a fifth word, which is the incorrect one found by the model\n",
    "  - a sixth term, which is the integer indicating the rank (or 0)\n",
    "* one correctly-solved analogy selected at random (in principle, four terms)."
   ]
  },
  {
   "cell_type": "code",
   "id": "composite-fundamentals",
   "metadata": {},
   "source": [
    "# Please write your Python code below and execute it.\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "broadband-contest",
   "metadata": {},
   "source": [
    "**2e.** Please write a function to compute the MRR score given a structure with correctly and incorrectly solved analogies, such as the one that is found in the results from `evaluate_word_analogies`.  The structure is not divided into categories.\n",
    "\n",
    "The Mean Reciprocal Rank (please use the [formula here](https://en.wikipedia.org/wiki/Mean_reciprocal_rank)) gives some credit for incorrectly solved analogies, in inverse proportion to the rank of the correct answer among the candidates.  This rank is 1 for correctly solved analogies (full credit), and 1/k (or 0) for incorrectly solved ones."
   ]
  },
  {
   "cell_type": "code",
   "id": "f5dc33e6",
   "metadata": {},
   "source": [
    "# Please define here the function that computes MRR from the information stored in analogy_scores\n",
    "def myMRR(analogies):\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "primary-breach",
   "metadata": {},
   "source": [
    "# Please test your MRR function by running the following code, which  displays the total number of analogy tasks, \n",
    "# the number of different categories (sections), the accuracy of the results (total number of correctly \n",
    "# solved analogies), and the MRR score of the results:\n",
    "print(\"Total number of analogies:\",  # The last dictionary is the total\n",
    "      len(analogy_scores[1][-1]['correct']) + \n",
    "      len(analogy_scores[1][-1]['incorrect']))\n",
    "print(\"Total number of categories:\", len(analogy_scores[1]) - 1) # the \"total\" is excluded \n",
    "print(f\"Overall accuracy: {analogy_scores[0]:.2f} and MRR: {myMRR(analogy_scores[1][-1]):.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bbd4662a",
   "metadata": {},
   "source": [
    "**2f.** When you have some time, please compute the accuracy and MRR and the total time for the entire `questions-words.txt` file.  Is the timing compatible with your estimate from (2b)?  What do you think about the difference between accuracy and MRR? "
   ]
  },
  {
   "cell_type": "code",
   "id": "07f2842d",
   "metadata": {},
   "source": [
    "# Please write your Python code below and execute it.\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a6be580",
   "metadata": {},
   "source": [
    "# Please write you answer here."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "economic-shore",
   "metadata": {},
   "source": [
    "## End of AdvNLP Lab 2\n",
    "Please make sure all cells have been executed, save this completed notebook, and upload it to Moodle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
