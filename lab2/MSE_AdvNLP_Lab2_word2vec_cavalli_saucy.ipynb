{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "quick-polls",
   "metadata": {},
   "source": [
    "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logo/0x150/1643104191/logo-mse.png)\n",
    "\n",
    "# AdvNLP Lab 2: Testing a pretrained word2vec model on analogy tasks\n",
    "\n",
    "**Objectives:**  experiment with *word vectors* from word2vec: test them on analogy tasks; use *accuracy and MRR* scores.\n",
    "\n",
    "**Useful documentation:** the [section on KeyedVectors in Gensim](https://radimrehurek.com/gensim/models/keyedvectors.html) and possibly the [section on word2vec](https://radimrehurek.com/gensim/models/word2vec.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-chase",
   "metadata": {},
   "source": [
    "## 1. Word2vec model trained on Google News\n",
    "**1a.** Please install the latest version of Gensim, preferably in a Conda environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "extreme-birthday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gensim\n",
      "Version: 4.3.3\n",
      "Summary: Python framework for fast Vector Space Modelling\n",
      "Home-page: https://radimrehurek.com/gensim/\n",
      "Author: Radim Rehurek\n",
      "Author-email: me@radimrehurek.com\n",
      "License: LGPL-2.1-only\n",
      "Location: C:\\Users\\a.cavalli\\AppData\\Local\\anaconda3\\envs\\AdvNLP\\Lib\\site-packages\n",
      "Requires: numpy, scipy, smart-open\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade gensim\n",
    "# You can run the following verification:\n",
    "!pip show gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "assumed-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, os, random\n",
    "from gensim import downloader\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import utils\n",
    "# help(gensim.models.word2vec) # take a look if needed\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-titanium",
   "metadata": {},
   "source": [
    "**1b.** Please download from Gensim the `word2vec-google-news-300` model, upon your first use.  Then, please write code to answer the following questions:\n",
    "* Where is the model stored on your computer and what is the file name?  You can store the absolute path in a variable called `path_to_model_file`.\n",
    "* What is the size of the corresponding file?  Please display the size in gigabytes with two decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "infectious-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model from Gensim (needed only the first time)\n",
    "#gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "# No need to store the returned value (uses a lot of memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "scheduled-binary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.cavalli/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "file size : 1.62 GB\n"
     ]
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "path_to_model_file = gensim.downloader.load(\"word2vec-google-news-300\", return_path=True)\n",
    "print(path_to_model_file)\n",
    "size_gb = os.path.getsize(path_to_model_file) / 2**30\n",
    "print('file size : %.2f GB' % size_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-insurance",
   "metadata": {},
   "source": [
    "**1c.** Please load the word2vec model as an instance of the class `KeyedVectors`, and store it in a variable called `wv_model`. \n",
    "What is, at this point, the memory size of the process corresponding to this notebook?  Simply write the value you obtain from any OS-specific utility that you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "instant-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.  Write the memory size on a commented line.\n",
    "wv_model = KeyedVectors.load_word2vec_format(path_to_model_file, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea3f7b-8e1e-4d0e-b8ab-0c23fdc2ce44",
   "metadata": {},
   "source": [
    "Memory size of the python process: 4152 MB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-flooring",
   "metadata": {},
   "source": [
    "**1d.** Please write the instructions that generate the answers to the following questions.\n",
    "* What is the size of the vocabulary of the `wv_model` model?  \n",
    "* What is the dimensionality of each word vector?  \n",
    "* What is the word corresponding to the vector in position 1234?  \n",
    "* What are the first 10 coefficients of the word vector for the word *pyramid*?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rubber-richardson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3000000\n",
      "Vector shape: (300,)\n",
      "Word in position 1234: learn\n",
      "\"pyramid\"'s first 10 coefficients: [ 0.00402832 -0.00260925  0.04296875  0.19433594 -0.03979492 -0.06445312\n",
      "  0.42773438 -0.18359375 -0.27148438 -0.12890625]\n"
     ]
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "print('Vocabulary size:', len(wv_model.key_to_index))\n",
    "print('Vector shape:', wv_model.get_vector('test').shape)\n",
    "print('Word in position 1234:', wv_model.index_to_key[1234])\n",
    "print('\"pyramid\"\\'s first 10 coefficients:', wv_model.get_vector('pyramid')[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-accessory",
   "metadata": {},
   "source": [
    "## 2. Solving analogies using word2vec trained on Google News\n",
    "In this section, you are going to use word vectors to solve analogy tasks provided with Gensim, such as \"What is to France what Rome is to Italy?\".  The predefined function in Gensim that evaluates a model on this task does not provide enough details, so you will need to make modifications to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642353b",
   "metadata": {},
   "source": [
    "**2a.** The analogy tasks are stored in a text file called `questions-words.txt` which is typically found in `C:\\Users\\YourNameHere\\.conda\\envs\\YourEnvNameHere\\Lib\\site-packages\\gensim\\test\\test_data`.  You can access it from here with Gensim as `datapath('questions-words.txt')`.  \n",
    "\n",
    "Please create a file called `questions-words-100.txt` with the first 100 lines from the original file.  Please run the evaluation task on this file, using the [documentation of the KeyedVectors class](https://radimrehurek.com/gensim/models/keyedvectors.html), then answer the following questions:\n",
    "* How many analogy tasks are there in your `questions-words-100.txt` file?\n",
    "* How many analogies were solved correctly and how many incorrectly?\n",
    "* What is the accuracy returned by `evaluate_word_analogies`?\n",
    "* How much time did it take to solve the analogies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ae43e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "(overall_score, sections) = wv_model.evaluate_word_analogies('questions-words-100.txt')\n",
    "end = time.time()\n",
    "duration = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb6adce7-73a0-44a0-be60-95afdba195e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time: 2.2704060077667236 seconds\n",
      "overall accuracy: 0.8080808080808081\n",
      "section capital-common-countries\n",
      "- correct: 80\n",
      "- incorrect: 19\n",
      "section Total accuracy\n",
      "- correct: 80\n",
      "- incorrect: 19\n"
     ]
    }
   ],
   "source": [
    "print(f'execution time: {duration} seconds')\n",
    "print('overall accuracy:', overall_score)\n",
    "for section in sections:\n",
    "    print('section', section['section'])\n",
    "    print('- correct:', len(section['correct']))\n",
    "    print('- incorrect:', len(section['incorrect']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1051fab-cc60-4847-a227-ffb70f19512c",
   "metadata": {},
   "source": [
    "The file contains 99 analogy tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da425e",
   "metadata": {},
   "source": [
    "**2b.** Please answer in writing the following questions:\n",
    "* What is the meaning of the first line of `questions-words-100.txt`?\n",
    "* How many analogies are there in the original `questions-words.txt`?\n",
    "* How much time would it take to solve the original set of analogies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cbed9f4-d28e-4d97-acbb-c25abc08c500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of analogies: 19544\n",
      "expected duration for all analogies: 448.2102526847762\n"
     ]
    }
   ],
   "source": [
    "analogies_count_all = 0\n",
    "with open(datapath('questions-words.txt')) as file:\n",
    "    for line in file:\n",
    "        if not line.startswith(':'):\n",
    "            analogies_count_all += 1\n",
    "\n",
    "print('total number of analogies:', analogies_count_all)\n",
    "\n",
    "analogies_count_subset = 99 # subset of questions-words-100.txt\n",
    "expected_duration_all = duration / analogies_count_subset * analogies_count_all\n",
    "print('expected duration for all analogies:', expected_duration_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41eda2d-9f6c-4b84-87ab-394eb066657f",
   "metadata": {},
   "source": [
    "- The first line represents the beginning of a section named \"capital-common-countries\"\n",
    "- The original `questions-words.txt` file contains 19544 analogies\n",
    "- It would take around 445 seconds (around 7 minutes) to execute the original set of analogies. Note: the first execution of the \"questions-words-100\" dataset takes longer ; the expected duration is computed on the duration of the second execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ea5d4",
   "metadata": {},
   "source": [
    "**2c.** The built-in function from Gensim has several weaknesses, which you will address here.  Please copy the source code of the function `evaluate_word_analogies` from the file `gensim\\models\\keyedvectors.py` and create here a new function which will improve the built-in one as follows.  The function will be called `my_evaluate_word_analogies` and you will also pass it the model as the first argument.  Overall, please proceed gradually and only make minimal modifications, to ensure you don't break the function.  It is important to first understand the structure of the result, `analogies_scores` and `sections`. \n",
    "\n",
    "* Modify the line where `section[incorrect]` is assembled in order to also add to each analogy the *incorrect guess* (i.e. what the model thought was the good answer, but got it wrong).\n",
    "\n",
    "* Modify the code so that when `section[incorrect]` is assembled, you also add the *rank of the correct answer* among the candidates returned by the system (after the incorrect guess).  If the correct answer is not present at all, then code the rank as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e054d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_evaluate_word_analogies(model, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False, similarity_function='most_similar'):\n",
    "    logger = gensim.logger\n",
    "    ok_keys = model.index_to_key[:restrict_vocab]\n",
    "    if case_insensitive:\n",
    "        ok_vocab = {k.upper(): model.get_index(k) for k in reversed(ok_keys)}\n",
    "    else:\n",
    "        ok_vocab = {k: model.get_index(k) for k in reversed(ok_keys)}\n",
    "    oov = 0\n",
    "    logger.info(\"Evaluating word analogies for top %i words in the model on %s\", restrict_vocab, analogies)\n",
    "    sections, section = [], None\n",
    "    quadruplets_no = 0\n",
    "    with utils.open(analogies, 'rb') as fin:\n",
    "        for line_no, line in enumerate(fin):\n",
    "            line = utils.to_unicode(line)\n",
    "            if line.startswith(': '):\n",
    "                # a new section starts => store the old section\n",
    "                if section:\n",
    "                    sections.append(section)\n",
    "                    model._log_evaluate_word_analogies(section)\n",
    "                section = {'section': line.lstrip(': ').strip(), 'correct': [], 'incorrect': []}\n",
    "            else:\n",
    "                if not section:\n",
    "                    raise ValueError(\"Missing section header before line #%i in %s\" % (line_no, analogies))\n",
    "                try:\n",
    "                    if case_insensitive:\n",
    "                        a, b, c, expected = [word.upper() for word in line.split()]\n",
    "                    else:\n",
    "                        a, b, c, expected = [word for word in line.split()]\n",
    "                except ValueError:\n",
    "                    logger.info(\"Skipping invalid line #%i in %s\", line_no, analogies)\n",
    "                    continue\n",
    "                quadruplets_no += 1\n",
    "                if a not in ok_vocab or b not in ok_vocab or c not in ok_vocab or expected not in ok_vocab:\n",
    "                    oov += 1\n",
    "                    if dummy4unknown:\n",
    "                        logger.debug('Zero accuracy for line #%d with OOV words: %s', line_no, line.strip())\n",
    "                        section['incorrect'].append((a, b, c, expected))\n",
    "                    else:\n",
    "                        logger.debug(\"Skipping line #%i with OOV words: %s\", line_no, line.strip())\n",
    "                    continue\n",
    "                original_key_to_index = model.key_to_index\n",
    "                model.key_to_index = ok_vocab\n",
    "                ignore = {a, b, c}  # input words to be ignored\n",
    "                predicted = None\n",
    "                guess_count = 0\n",
    "                right_prediction_rank = 0\n",
    "                \n",
    "                # find the most likely prediction using 3CosAdd (vector offset) method\n",
    "                # TODO: implement 3CosMul and set-based methods for solving analogies\n",
    "    \n",
    "                sims = model.most_similar(positive=[b, c], negative=[a], topn=5, restrict_vocab=restrict_vocab)\n",
    "                model.key_to_index = original_key_to_index\n",
    "                for element in sims:\n",
    "                    current = element[0].upper() if case_insensitive else element[0]\n",
    "                    if current not in ok_vocab or current in ignore:\n",
    "                        continue\n",
    "                        \n",
    "                    guess_count += 1 # count only valid similarities\n",
    "                    if predicted is None:\n",
    "                        predicted = current\n",
    "                        if predicted == expected:\n",
    "                            break # 1st prediction is correct\n",
    "                        else:\n",
    "                            logger.debug(\"%s: expected %s, predicted %s\", line.strip(), expected, predicted)\n",
    "                    elif current == expected:\n",
    "                        right_prediction_rank = guess_count\n",
    "                        logger.debug(\"%s: expected %s, guessed right at rank %i\", line.strip(), expected, right_prediction_rank)\n",
    "                        break # 1st prediction wasn't correct, but the n-th prediction is correct (see right_prediction_rank)\n",
    "                        \n",
    "                if predicted == expected:\n",
    "                    section['correct'].append((a, b, c, expected))\n",
    "                else:\n",
    "                    section['incorrect'].append((a, b, c, expected, predicted, right_prediction_rank))\n",
    "    if section:\n",
    "        # store the last section, too\n",
    "        sections.append(section)\n",
    "        model._log_evaluate_word_analogies(section)\n",
    "    \n",
    "    total = {\n",
    "        'section': 'Total accuracy',\n",
    "        'correct': list(itertools.chain.from_iterable(s['correct'] for s in sections)),\n",
    "        'incorrect': list(itertools.chain.from_iterable(s['incorrect'] for s in sections)),\n",
    "    }\n",
    "    \n",
    "    oov_ratio = float(oov) / quadruplets_no * 100\n",
    "    logger.info('Quadruplets with out-of-vocabulary words: %.1f%%', oov_ratio)\n",
    "    if not dummy4unknown:\n",
    "        logger.info(\n",
    "            'NB: analogies containing OOV words were skipped from evaluation! '\n",
    "            'To change this behavior, use \"dummy4unknown=True\"'\n",
    "        )\n",
    "    analogies_score = model._log_evaluate_word_analogies(total)\n",
    "    sections.append(total)\n",
    "    # Return the overall score and the full lists of correct and incorrect analogies\n",
    "    return analogies_score, sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fec19",
   "metadata": {},
   "source": [
    "**2d.** Please run the `my_evaluate_word_analogies` function on `questions-words-100.txt` and then write instructions to display, from the results stored in `analogy_scores`:\n",
    "* one incorrectly-solved analogy (selected at random), including also the error made by the model and the rank of the correct answer, thus adding:\n",
    "  - a fifth word, which is the incorrect one found by the model\n",
    "  - a sixth term, which is the integer indicating the rank (or 0)\n",
    "* one correctly-solved analogy selected at random (in principle, four terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "composite-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "analogy_scores = my_evaluate_word_analogies(wv_model, 'questions-words-100.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd19b907-4f00-4501-bb4a-eae9fa68865a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random incorrect analogy: ('BAGHDAD', 'IRAQ', 'CANBERRA', 'AUSTRALIA', 'MR_RUDD', 2)\n",
      "random correct analogy: ('ATHENS', 'GREECE', 'BERLIN', 'GERMANY')\n"
     ]
    }
   ],
   "source": [
    "(overall_score, sections) = analogy_scores\n",
    "section = sections[0]\n",
    "\n",
    "print('random incorrect analogy:', random.choice(section['incorrect']))\n",
    "print('random correct analogy:', random.choice(section['correct']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-contest",
   "metadata": {},
   "source": [
    "**2e.** Please write a function to compute the MRR score given a structure with correctly and incorrectly solved analogies, such as the one that is found in the results from `evaluate_word_analogies`.  The structure is not divided into categories.\n",
    "\n",
    "The Mean Reciprocal Rank (please use the [formula here](https://en.wikipedia.org/wiki/Mean_reciprocal_rank)) gives some credit for incorrectly solved analogies, in inverse proportion to the rank of the correct answer among the candidates.  This rank is 1 for correctly solved analogies (full credit), and 1/k (or 0) for incorrectly solved ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5dc33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please define here the function that computes MRR from the information stored in analogy_scores\n",
    "def myMRR(analogies):\n",
    "    correct_count = len(analogies['correct'])\n",
    "    incorrect_count = len(analogies['incorrect'])\n",
    "    total_count = correct_count + incorrect_count\n",
    "    \n",
    "    mrr = correct_count\n",
    "    for incorrect in analogies['incorrect']:\n",
    "        rank = incorrect[-1]\n",
    "        if rank > 0:\n",
    "            mrr += 1 / rank\n",
    "    return mrr / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "primary-breach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of analogies: 99\n",
      "Total number of categories: 1\n",
      "Overall accuracy: 0.81 and MRR: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Please test your MRR function by running the following code, which  displays the total number of analogy tasks, \n",
    "# the number of different categories (sections), the accuracy of the results (total number of correctly \n",
    "# solved analogies), and the MRR score of the results:\n",
    "print(\"Total number of analogies:\",  # The last dictionary is the total\n",
    "      len(analogy_scores[1][-1]['correct']) + \n",
    "      len(analogy_scores[1][-1]['incorrect']))\n",
    "print(\"Total number of categories:\", len(analogy_scores[1]) - 1) # the \"total\" is excluded \n",
    "print(f\"Overall accuracy: {analogy_scores[0]:.2f} and MRR: {myMRR(analogy_scores[1][-1]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4662a",
   "metadata": {},
   "source": [
    "**2f.** When you have some time, please compute the accuracy and MRR and the total time for the entire `questions-words.txt` file.  Is the timing compatible with your estimate from (2b)?  What do you think about the difference between accuracy and MRR? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07f2842d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time: 401.6764090061188 seconds\n"
     ]
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "start = time.time()\n",
    "analogy_scores = my_evaluate_word_analogies(wv_model, datapath('questions-words.txt'))\n",
    "end = time.time()\n",
    "duration = end - start\n",
    "print(f'execution time: {duration} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274dfd16-630d-40ea-97cc-2411fe92b7d5",
   "metadata": {},
   "source": [
    "The execution time (401 seconds) is close to the expected one (448 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bed6a3e0-8f68-4e4b-a2d4-879505f9df70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of analogies: 19330\n",
      "Total number of categories: 14\n",
      "Overall accuracy: 0.74 and MRR: 0.79\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of analogies:\",  # The last dictionary is the total\n",
    "      len(analogy_scores[1][-1]['correct']) + \n",
    "      len(analogy_scores[1][-1]['incorrect']))\n",
    "print(\"Total number of categories:\", len(analogy_scores[1]) - 1) # the \"total\" is excluded \n",
    "print(f\"Overall accuracy: {analogy_scores[0]:.2f} and MRR: {myMRR(analogy_scores[1][-1]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3365bdcb-c5a9-446a-b02f-5a94b9b2e8b5",
   "metadata": {},
   "source": [
    "For the accuracy of one task, we only considers whether the task was performed correctly: a correct analogy gets a score of 1 and and incorrect analogy gets a score of 0. The overall accuracy is the sum of the individual scores divided by the number of tasks.\n",
    "\n",
    "For an incorrect analogy, the MRR score also considers the other results proposed by the model (from rank number 2). The MRR score for a task gives a score of 1 for a correct analogy (as for the accuracy), and a score between 0 and 0.5 for an incorrect analogy, based on the rank of the correct analogy among the proposed results. As for the overall accuracy, the overall MRR score is the sum of the individual scores divided by the number of tasks.\n",
    "\n",
    "Thus, the minimum MRR score we can get is equal to the accuracy. We get this minimum score when, among all proposed results of an incorrect analogy, the model never proposed the correct one.\n",
    "If the model proposes a correct result for at least one incorrect analogy, the MRR score will be greater than the accuracy.\n",
    "For a given accuracy, the maximum MRR score a model can get is `accuracy + (1 - accuracy) * 0.5`. This MRR score can be reached when, for all incorrect analogies, the correct result was the second proposed result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-shore",
   "metadata": {},
   "source": [
    "## End of AdvNLP Lab 2\n",
    "Please make sure all cells have been executed, save this completed notebook, and upload it to Moodle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AdvNLP",
   "language": "python",
   "name": "advnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
